volume255 <- voxels255 * cubicUmPerVoxel
totalVolume <- volume255 + volume000
matrixVolume <- volume255
# read morphology data (MorphoLibJ > Analysis > Particle Analysis 3D)
# organize into a new data table called morpho
morpho <- read_tsv(paste0('./image-data/',baseName,'-lbl-morpho.tsv'),col_names=TRUE)
# read bounding box data (MorphoLibJ > Analysis > Bounding Box 3D)
# convert voxels to microns and combine with morphology data
# add to morphology data table
bounds <- read_tsv(paste0('./image-data/',baseName,'-lbl-bounds.tsv'),col_names=TRUE) %>%
mutate(xBox = (XMax-XMin)*umPerVoxel,
yBox = (YMax-YMin)*umPerVoxel,
zBox = (ZMax-ZMin)*umPerVoxel) %>%
select(xBox,yBox,zBox)
morpho <- bind_cols(morpho,bounds)
# finally, add the total matrix volume in the last column
morpho <- morpho %>%
mutate(vMatrix = matrixVolume)
# prepend first and second columns to identify this data set
morpho <- morpho %>%
mutate(scanID = baseName,
scanDesc = description) %>%
select(scanID, scanDesc, everything(), -X1)
return(morpho)
}
xct <-      getSegmentation('scan01','SE508') %>%
bind_rows(getSegmentation('scan02','SE508ELI')) %>%
bind_rows(getSegmentation('scan03','SE508ELI')) %>%
filter(Volume > cutoffVolume) %>%
mutate(vPerCuMm = Volume / 1e9)
xct <- xct %>%
mutate(scanID = parse_factor(scanID, levels = factorID),
scanDesc = parse_factor(scanDesc, levels = factorDesc))
p.count <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID)) +
facet_grid(scanID ~ .) +
xlab('inclusion size (cubic micron)') +
ylab('inclusion count') +
ggtitle('inclusion count by size')
plot(p.count)
p.vol <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID,weight=Volume)) +
facet_grid(scanID ~ .) +
xlab('inclusion size (cubic micron)') +
ylab('sum of inclusion volume (cubic micron)') +
ggtitle('inclusion volume by size')
plot(p.vol)
p.count.ll <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID)) +
facet_grid(scanID ~ .) +
scale_y_log10() +
scale_x_log10(limits = c(1,NA)) +
xlab('inclusion size (cubic micron)') +
ylab('inclusion count') +
ggtitle('inclusion count by size')
plot(p.count.ll)
p.vol.ll <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID,weight=Volume)) +
facet_grid(scanID ~ .) +
scale_y_log10() +
scale_x_log10(limits = c(1,NA)) +
xlab('inclusion size (cubic micron)') +
ylab('sum of inclusion volume (cubic micron)') +
ggtitle('inclusion volume by size')
plot(p.vol.ll)
inclusionCount <- count(xct, scanID)
countByScan <- xct %>%
group_by(scanID,scanDesc) %>%
summarize(vMatrix = max(vMatrix)) %>%
bind_cols(inclusionCount[,2]) %>%
mutate(nPerUm3 = n/vMatrix,
nPerMm3 = nPerUm3 * 1e9)
print(countByScan)
countByDesc <- xct %>%
group_by(scanID,scanDesc) %>%
summarize(vMatrix = max(vMatrix)) %>%
bind_cols(inclusionCount[,2]) %>%
ungroup() %>%
group_by(scanDesc) %>%
summarize(vMatrix = sum(vMatrix),
n = sum(n)) %>%
mutate(nPerUm3 = n/vMatrix,
nPerMm3 = nPerUm3 * 1e9)
print(countByDesc)
source('~/R-workspace/nitinol-design-concepts/210-xct-methods/xct-process-imagej-results.R', echo=TRUE)
v.matl
v.plane
v.curoff
v.cutoff
v.nPerMm3
v.mu
v.s
dfGumbel
dfGumbel <- bind_cols(as_tibble(v.matl),as_tibble(v.plane),
as_tibble(v.cutoff),
as_tibble(v.nPerMm3), as_tibble(v.mu),as_tibble(v.s))
library(fitdistrplus) # for fitting Gumbel distribution
library(tidyverse)    # http://r4ds.had.co.nz/
rm(list=ls())
cutoffVolume <- 8 # filter out particles less than this value (cubic microns)
umPerVoxel <- 0.500973555972 # voxel edge size from scan (microns)
cubicUmPerVoxel <- (umPerVoxel^3)
factorID <- c('scan01','scan02','scan03') # valid file name prefixes
factorDesc <-c('SE508','SE508ELI') # valid descriptions
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
getSegmentation <- function(baseName, description){
# read volume information from histogram file
# total volume is about 0.42mm^3, of which 0.25mm^3 is matrix
# exact volumes vary depending on the mask used for each scan
histogram000 <- read_tsv(paste0('./image-data/',baseName,'-mask-histogram.tsv'),skip=1,col_names=FALSE)
histogram255 <- read_tsv(paste0('./image-data/',baseName,'-mask-histogram.tsv'),skip=256,col_names=FALSE)
voxels000 <- histogram000[[1,2]]
volume000 <- voxels000 * cubicUmPerVoxel
voxels255 <- histogram255[[1,2]]
volume255 <- voxels255 * cubicUmPerVoxel
totalVolume <- volume255 + volume000
matrixVolume <- volume255
# read morphology data (MorphoLibJ > Analysis > Particle Analysis 3D)
# organize into a new data table called morpho
morpho <- read_tsv(paste0('./image-data/',baseName,'-lbl-morpho.tsv'),col_names=TRUE)
# read bounding box data (MorphoLibJ > Analysis > Bounding Box 3D)
# convert voxels to microns and combine with morphology data
# add to morphology data table
bounds <- read_tsv(paste0('./image-data/',baseName,'-lbl-bounds.tsv'),col_names=TRUE) %>%
mutate(xBox = (XMax-XMin)*umPerVoxel,
yBox = (YMax-YMin)*umPerVoxel,
zBox = (ZMax-ZMin)*umPerVoxel) %>%
select(xBox,yBox,zBox)
morpho <- bind_cols(morpho,bounds)
# finally, add the total matrix volume in the last column
morpho <- morpho %>%
mutate(vMatrix = matrixVolume)
# prepend first and second columns to identify this data set
morpho <- morpho %>%
mutate(scanID = baseName,
scanDesc = description) %>%
select(scanID, scanDesc, everything(), -X1)
return(morpho)
}
xct <-      getSegmentation('scan01','SE508') %>%
bind_rows(getSegmentation('scan02','SE508ELI')) %>%
bind_rows(getSegmentation('scan03','SE508ELI')) %>%
filter(Volume > cutoffVolume) %>%
mutate(vPerCuMm = Volume / 1e9)
xct <- xct %>%
mutate(scanID = parse_factor(scanID, levels = factorID),
scanDesc = parse_factor(scanDesc, levels = factorDesc))
p.count <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID)) +
facet_grid(scanID ~ .) +
xlab('inclusion size (cubic micron)') +
ylab('inclusion count') +
ggtitle('inclusion count by size')
plot(p.count)
p.vol <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID,weight=Volume)) +
facet_grid(scanID ~ .) +
xlab('inclusion size (cubic micron)') +
ylab('sum of inclusion volume (cubic micron)') +
ggtitle('inclusion volume by size')
plot(p.vol)
p.count.ll <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID)) +
facet_grid(scanID ~ .) +
scale_y_log10() +
scale_x_log10(limits = c(1,NA)) +
xlab('inclusion size (cubic micron)') +
ylab('inclusion count') +
ggtitle('inclusion count by size')
plot(p.count.ll)
p.vol.ll <- ggplot(xct) +
geom_histogram(aes(Volume,fill=scanID,weight=Volume)) +
facet_grid(scanID ~ .) +
scale_y_log10() +
scale_x_log10(limits = c(1,NA)) +
xlab('inclusion size (cubic micron)') +
ylab('sum of inclusion volume (cubic micron)') +
ggtitle('inclusion volume by size')
plot(p.vol.ll)
inclusionCount <- count(xct, scanID)
countByScan <- xct %>%
group_by(scanID,scanDesc) %>%
summarize(vMatrix = max(vMatrix)) %>%
bind_cols(inclusionCount[,2]) %>%
mutate(nPerUm3 = n/vMatrix,
nPerMm3 = nPerUm3 * 1e9)
print(countByScan)
countByDesc <- xct %>%
group_by(scanID,scanDesc) %>%
summarize(vMatrix = max(vMatrix)) %>%
bind_cols(inclusionCount[,2]) %>%
ungroup() %>%
group_by(scanDesc) %>%
summarize(vMatrix = sum(vMatrix),
n = sum(n)) %>%
mutate(nPerUm3 = n/vMatrix,
nPerMm3 = nPerUm3 * 1e9)
print(countByDesc)
xct <- xct %>%
mutate(xyArea = Volume / zBox, # area projected in XY plane (transverse)
xzArea = Volume / yBox, # area projected in XZ plane (longitudinal)
yzArea = Volume / xBox, # area projected in YZ plane (longitudinal)
rootXyArea = xyArea^(1/2),
rootXzArea = xzArea^(1/2),
rootYzArea = yzArea^(1/2))
dgumbel <- function(x,mu,s){ # PDF
exp((mu - x)/s - exp((mu - x)/s))/s
}
pgumbel <- function(q,mu,s){ # CDF
exp(-exp(-((q - mu)/s)))
}
qgumbel <- function(p, mu, s){ # quantile function
mu-s*log(-log(p))
}
xct.se508 <- filter(xct,scanDesc=='SE508')
xct.eli <- filter(xct,scanDesc=='SE508ELI')
gumbelFit <- function(vector){
fit <- fitdist(vector, "gumbel",
start=list(mu=4, s=1),
method="mle")
return(fit)
}
gumbel.se508.xz <- gumbelFit(xct.se508$rootXzArea)
gumbel.se508.yz <- gumbelFit(xct.se508$rootYzArea)
gumbel.se508.xy <- gumbelFit(xct.se508$rootXyArea)
gumbel.eli.xz <- gumbelFit(xct.eli$rootXzArea)
gumbel.eli.yz <- gumbelFit(xct.eli$rootYzArea)
gumbel.eli.xy <- gumbelFit(xct.eli$rootXyArea)
par(cex=1.2, bg="white")
plot(gumbel.se508.xy, lwd=2, col="steelblue")
plot(gumbel.se508.yz, lwd=2, col="steelblue")
plot(gumbel.se508.xz, lwd=2, col="steelblue")
plot(gumbel.eli.xy, lwd=2, col="steelblue")
plot(gumbel.eli.yz, lwd=2, col="steelblue")
plot(gumbel.eli.xz, lwd=2, col="steelblue")
v.matl <- c(rep('se508',3),rep('eli',3))
n.SE508 <- countByDesc[countByDesc$scanDesc=='SE508','nPerMm3'][[1]]
n.SE508ELI <- countByDesc[countByDesc$scanDesc=='SE508ELI','nPerMm3'][[1]]
v.nPerMm3 <- c(rep(n.SE508,3),
rep(n.SE508ELI,3))
v.plane <- c(rep(c('xy','yz','xz'),3))
v.plane <- c(rep(c('xy','yz','xz'),2))
v.cutoff <- c(rep(cutoffVolume,6))
gumbelMu <- function(gumbelModel){
return(gumbelModel$estimate[[1]])
}
gumbelS <- function(gumbelModel){
return(gumbelModel$estimate[[2]])
}
v.mu <- c(gumbelMu(gumbel.se508.xy),
gumbelMu(gumbel.se508.yz),
gumbelMu(gumbel.se508.xz),
gumbelMu(gumbel.eli.xy),
gumbelMu(gumbel.eli.yz),
gumbelMu(gumbel.eli.xz))
v.s <- c(gumbelS(gumbel.se508.xy),
gumbelS(gumbel.se508.yz),
gumbelS(gumbel.se508.xz),
gumbelS(gumbel.eli.xy),
gumbelS(gumbel.eli.yz),
gumbelS(gumbel.eli.xz))
dfGumbel <- bind_cols(as_tibble(v.matl),as_tibble(v.plane),
as_tibble(v.cutoff),
as_tibble(v.nPerMm3), as_tibble(v.mu),as_tibble(v.s))
colnames(dfGumbel) <- c('matl', 'plane', 'cutoff', 'nPerMm3', 'mu', 's')
dir.create('./out/gumbel', showWarnings = FALSE)
write_csv(dfGumbel, path = paste('./out/','gumbelParameters',
'.csv',sep = ''), na='0')
dir.create('./out', showWarnings = FALSE)
write_csv(dfGumbel, path = paste('./out/','gumbelParameters',
'.csv',sep = ''), na='0')
source('~/R-workspace/nitinol-design-concepts/210-xct-methods/xct-process-imagej-results.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/210-xct-methods/xct-process-imagej-results.R', echo=TRUE)
print(countByDesc)
source('~/R-workspace/nitinol-design-concepts/210-xct-methods/xct-process-imagej-results.R', echo=TRUE)
bounds
source('~/R-workspace/nitinol-design-concepts/210-xct-methods/xct-process-imagej-results.R', echo=TRUE)
print(countByScan)
dfGumbel
library(tidyverse) # http://r4ds.had.co.nz
read_table('open-frame-fatigue-v25mm-9pct.rpt', skip=19)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
results <- read_table('open-frame-fatigue-v25mm-9pct.rpt', skip=19)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
results <- read_table('open-frame-fatigue-v25mm-9pct.rpt', skip=19)
results <- read_table('open-frame-fatigue-v25mm-9pct.rpt', skip=19)
results <- read_table('open-frame-fatigue-v25mm-9pct.rpt', skip=19)
results <- read_table('open-frame-fatigue-v25mm-9pct.rpt',
skip=19, col_names = FALSE)
colnames(results) <- c('elNum','intPt','eMean','eAmp')
pointCloud <- ggplot(results,aes(x=eMean,y=eAmp)) +
geom_point()
plot(pointCloud)
pointCloud <- ggplot(results,aes(x=abs(eMean),y=abs(eAmp))) +
geom_point()
plot(pointCloud)
pointCloud <- ggplot(results,aes(x=abs(eMean),y=abs(eAmp))) +
geom_point() +
ylab('strain amplitude') +
xlab('mean strain') +
scale_y_continuous(labels=percent) +
scale_x_continuous(labels=percent) +
ggtitle('point clous')
pointCloud <- ggplot(results,aes(x=abs(eMean),y=abs(eAmp))) +
geom_point() +
ylab('strain amplitude') +
xlab('mean strain') +
scale_y_continuous(labels = scales::percent) +
scale_x_continuous(labels = scales::percent) +
ggtitle('point clous')
plot(pointCloud)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/120-open-frame-fatigue/point-cloud.R', echo=TRUE)
list.files(path = "./", full.names = TRUE, recursive = FALSE)
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
list.files(path = "./", full.names = TRUE, recursive = FALSE)
files <- list.files(path = "./", pattern = pattern = "\\.ivol.csv$",
full.names = TRUE, recursive = FALSE)
files <- list.files(path = "./", pattern = "\\.ivol.csv$",
full.names = TRUE, recursive = FALSE)
list.files(path = "./", pattern = "\\.ivol.csv$",
full.names = TRUE, recursive = FALSE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
rm(list=ls())
library(tidyverse) # http://r4ds.had.co.nz
library(forcats) # http://r4ds.had.co.nz/factors.html
fileSelect <- 1
savePdfPng <- function(name){
ggsave(paste('pdf/',ident,'-',name,'.pdf',sep=''),
width=8,height=6)
ggsave(paste('png/',ident,'-',name,'.png',sep=''),
width=8,height=6,dpi = 300)
}
limitEM <- 0.015
limitEA <- 0.015
limitSWT <- 600
limitSM <- NA # 500
limitSA <- NA
symmetry <- 12
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
files <- list.files(path = "./", pattern = "\\.ivol.csv$",
full.names = TRUE, recursive = FALSE)
resultsFile <- files[fileSelect]
baseName <- basename(resultsFile)
baseName <- substring(baseName,1,nchar(baseName)-9)
ident <- baseName
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/mergeResults.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
p0 <- ggplot(data=df) +
geom_point(mapping = aes(x=cycEM,y=cycEA,color=cyc.Tension), alpha=0.2) +
scale_x_continuous(limits = c(0,limitEM) , labels = scales::percent) +
scale_y_continuous(limits = c(NA,limitEA), labels = scales::percent) +
scale_color_brewer(palette="Set1") +
xlab(expression(epsilon["m"])) +
ylab(expression(epsilon["a"])) +
ggtitle('Raw point cloud, highlighting points in hydrostatic tension/compression',baseName)
plot(p0)
savePdfPng('p00')
# postprocessFEA.R
#
# post-process FEA results. create point clouds. calculate volume of
# material exceeding strain amplitude thresholds. calculate volume of
# material transforming to martensite, cycling between A and M ,etc.
#
# IN:  .CSV file in current folder, created by ivolResults.py
# OUT: .PDF and PNG figures to ./pdf and ./png folders
#      .CSV file to ./out folder
#
# NEXT: mergeResults.R to combine ./out/*.csv into one summary .csv
# Clear environment and load packages -----------------------------------------
rm(list=ls())
library(tidyverse) # http://r4ds.had.co.nz
library(forcats) # http://r4ds.had.co.nz/factors.html
# File selection --------------------------------------------------------------
# index number of the file to process
# manually change this from 1 to (qty of files) and re-source this script
# file must end with ".ivol.csv"
fileSelect <- 1
# Helper functions  -----------------------------------------------------------
# function to save file as PDF and PNG
# PDF files are generally preferred, but
# get large and unwieldy with many points are plotted
savePdfPng <- function(name){
ggsave(paste('pdf/',ident,'-',name,'.pdf',sep=''),
width=8,height=6)
ggsave(paste('png/',ident,'-',name,'.png',sep=''),
width=8,height=6,dpi = 300)
}
# Setup -----------------------------------------------------------------------
# strain axis limits. replace NA with desired values.
limitEM <- NA
limitEA <- NA
limitSWT <- NA
# stress axis limits. replace NA with desired values.
limitSM <- NA
limitSA <- NA
# symmetry factor. if the FEA model represents 1/4 of the full component,
# sett this to 4. all volumes are multiplied by this value.
symmetry <- 1
# Read files ------------------------------------------------------------------
# set working directory to location of this script
setwd(dirname(rstudioapi::getActiveDocumentContext()$path))
# create a list of files in the specified directory
files <- list.files(path = "./", pattern = "\\.ivol.csv$",
full.names = TRUE, recursive = FALSE)
# read selected file
resultsFile <- files[fileSelect]
baseName <- basename(resultsFile)
baseName <- substring(baseName,1,nchar(baseName)-9)
#ident <- substring(baseName,17,24)
ident <- baseName
# create folders for results if they do not already exist
dir.create('pdf', showWarnings = FALSE)
dir.create('png', showWarnings = FALSE)
dir.create('out', showWarnings = FALSE)
# define column types
# readr will usually guess these correctly, but can get confused
# if many rows are 0 followed by some rows with a decimal value like 0.123456
# as is often the case for ldM and ulM (martensite volume fraction)
columnTypes = list(col_integer(), # el     = element number
col_integer(), # ip     = integration point
col_double(),  # cycEM  = maximum principal cyclic mean strain
col_double(),  # cycEA  = absolute maximum principal cyclic strain amplitude
col_double(),  # cycTau = cyclic maximum shear strain
col_double(),  # cycSM  = maximum principal cyclic mean stress
col_double(),  # cycSA  = absolute maximum principal cyclic stress amplitude
col_double(),  # preE   = pre-strain (strain conditioning, e.g. strain during crimping)
col_double(),  # preS   = pre-stress (stress conditioning, e.g. stress during crimping)
col_double(),  # preP   = hydrostatic pressure during pre-conditioning (compression positive, tension negative)
col_double(),  # preM   = volume fraction martensite during pre-conditioning
col_double(),  # preV   = integration point volume during pre-conditioning
col_double(),  # ldE    = maximum principal strain during loading frame of fatigue cycle
col_double(),  # ldTau  = maximum shear strain during loading frame of fatigue cycle
col_double(),  # ldS    = maximum principal stress during loading frame of fatigue cycle
col_double(),  # ldP    = hydrostatic pressure during loading frame of fatigue cycle
col_double(),  # ldM    = volume fraction martensite during loading frame of fatigue cycle
col_double(),  # ldV    = integration point volume during loading frame of fatigue cycle
col_double(),  # ulE    = maximum principal strain during unloading frame of fatigue cycle
col_double(),  # ulTau  = maximum shear strain during unloading frame of fatigue cycle
col_double(),  # ulS    = maximum principal stress during unloading frame of fatigue cycle
col_double(),  # ulP    = hydrostatic pressure during unloading frame of fatigue cycle
col_double(),  # ulM    = volume fraction martensite during unloading frame of fatigue cycle
col_double(),  # ulV    = integration point volume during unloading frame of fatigue cycle
col_double(),  # ldS11  = loading stress in material 1 direction (r)
col_double(),  # ldS22  = loading stress in material 2 direction (theta)
col_double(),  # ldS33  = loading stress in material 3 direction (Z)
col_double(),  # ulS11  = unloading stress in material 1 direction (r)
col_double(),  # ulS22  = unloading stress in material 2 direction (theta)
col_double(),  # ulS33  = unloading stress in material 3 direction (Z)
col_double(),  # ldE11  = loading strain in material 1 direction (r)
col_double(),  # ldE22  = loading strain in material 2 direction (theta)
col_double(),  # ldE33  = loading strain in material 3 direction (Z)
col_double(),  # ulE11  = unloading strain in material 1 direction (r)
col_double(),  # ulE22  = unloading strain in material 2 direction (theta)
col_double()   # ulE33  = unloading strain in material 3 direction (Z)
)
# create a data frame (table) from the CSV
# skip the header rows at the top of the file
# use the strings in the next row as column names
# explicitly define column types from list defined above
df <- read_csv(resultsFile, skip=46, col_names = TRUE, col_types = columnTypes)
# Pre-process the data --------------------------------------------------------
# adjust volume values to account for symmetry
# ("mutate" adds a new column to the data frame)
df <- df %>%
mutate(preV = preV * symmetry,
ldV  = ldV * symmetry,
ulV  = ulV * symmetry)
# Fatigue fractures will not propogate in compression.
# We know the pressure at element at two points: the loaded and unloaded cyclic frame.
# negative pressure = hydrostatic tension, positive pressure = hydrostatic compression
# "AND" condition: a point is in tension in both frames
# "OR" condition: a point is in tension in one of the two frames
# "MEAN" condition: average of load and unload pressure is in tension
# In some conditions, the unload cycle can move some points into compression,
# so if we strictly require the "AND" condition, we will discard potentially important points.
# Alternatively, the "OR" condition can allow inclusion of many points that are
# in tension for only a small part of the cycle.
# if a point is in tension in at least one of the two frames (OR condition),
# negative pressure is tension ... TRUE if in tension at any part of fatigue cycle
df <- df %>%
mutate(cyc.Tension.OR    = if_else(((ldP <= 0) | (ulP <= 0)), TRUE, FALSE),
cyc.Tension.AND   = if_else(((ldP <= 0) & (ulP <= 0)), TRUE, FALSE),
cyc.Tension.MEAN  = if_else((0.5*(ldP+ulP)) <= 0, TRUE, FALSE)) %>%
rowwise() %>%
mutate(cyc.Tension.10pctl = if_else( (min(ldP,ulP)+(abs(ldP-ulP)/10) ) <= 0,TRUE,FALSE),
cyc.Tension.50pctl = if_else( (0.5*(ldP+ulP) ) <= 0,TRUE,FALSE),
cyc.Tension.90pctl = if_else( (max(ldP,ulP)-(abs(ldP-ulP)/10) ) <= 0,TRUE,FALSE))
# the 90th percentile value is closest to the most conservative "OR" condition,
# but allows inclusion of points that slip just into compression for part of the
# unloading cycle. Experiment with different options by changing the next line.
df$cyc.Tension <- df$cyc.Tension.90pctl
# Generate raw point cloud ----------------------------------------------------
p0 <- ggplot(data=df) +
geom_point(mapping = aes(x=cycEM,y=cycEA,color=cyc.Tension), alpha=0.2) +
scale_x_continuous(limits = c(0,limitEM) , labels = scales::percent) +
scale_y_continuous(limits = c(NA,limitEA), labels = scales::percent) +
scale_color_brewer(palette="Set1") +
xlab(expression(epsilon["m"])) +
ylab(expression(epsilon["a"])) +
ggtitle('Raw point cloud, highlighting points in hydrostatic tension/compression',baseName)
plot(p0)
savePdfPng('p00')
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/mergeResults.R', echo=TRUE)
source('~/R-workspace/nitinol-design-concepts/125-volumetric-analysis/postprocessFEA.R', echo=TRUE)
